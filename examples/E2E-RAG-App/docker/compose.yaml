services:
  chromadb:
    image: chromadb/chroma:latest
    network_mode: "host"
    volumes:
      - ${DOC_PATH}:/root/rag_data/
      - ./chroma_start.sh:/root/chroma_start.sh
    ports:
      - "6000:6000"
    entrypoint: bash /root/chroma_start.sh
  ollama:
    image: ollama/ollama:latest
    network_mode: "host"
    environment:
      - MODEL_NAME=${MODEL_NAME}
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=127.0.0.1:14343
    volumes:
      - ollama:/root/.ollama # this solution synchronizes with the docker volume and loads the model rocket fast
      - ./ollama_start.sh:/root/ollama_start.sh
      - ./RAG_service.json:/root/RAG_service.json
    ports:
      - "14343:14343"
    tty: true
    restart: always
    entrypoint: ["bash", "/root/ollama_start.sh"]
  llamastack:
    image: llamastack/distribution-ollama:test-0.0.53rc8
    network_mode: "host"
    tty: true
    volumes:
      - ~/.llama:/root/.llama
      # Link to ollama run.yaml file
      - ./llama_stack_run.yaml:/root/my-run.yaml
      - ../../E2E-RAG-App:/root/E2E-RAG-App
      - ${DOC_PATH}:/root/rag_data/
      - ./llama_stack_start.sh:/root/llama_stack_start.sh
    ports:
      - "5000:5000" # for llama-stack
      - "7860:7860" # for UI
    # Hack: wait for ollama server to start before starting docker
    entrypoint: ["bash", "/root/llama_stack_start.sh"]
    #entrypoint: bash -c "sleep 60; python -m llama_stack.distribution.server.server --yaml_config /root/my-run.yaml"
    deploy:
      restart_policy:
        condition: "no"

volumes:
  ollama:
