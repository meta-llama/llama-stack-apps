services:
  ollama:
    image: ollama/ollama:latest
    env_file: "docqa_env"
    network_mode: "host"
    environment:
      - MODEL_NAME=${MODEL_NAME}
      - OLLAMA_KEEP_ALIVE=99h
    volumes:
      - ollama:/root/.ollama # this solution synchronizes with the docker volume and loads the model rocket fast
      - ./ollama_start.sh:/root/ollama_start.sh
    ports:
      - "11434:11434"
    tty: true
    restart: always
    entrypoint: ["bash", "/root/ollama_start.sh"]
  llamastack:
    image: llamastack/distribution-ollama:0.0.63
    env_file: "docqa_env"
    network_mode: "host"
    environment:
      - USE_DOCKER=true
    tty: true
    volumes:
      - ~/.llama:/root/.llama
      # Link to ollama run.yaml file
      - ../../DocQA:/root/DocQA
      - ${DOC_PATH}:/root/rag_data/
      - ./llama_stack_start.sh:/root/llama_stack_start.sh
    ports:
      - "${GRADIO_SERVER_PORT}:${GRADIO_SERVER_PORT}" # for UI
    entrypoint: ["bash", "/root/llama_stack_start.sh"]
    deploy:
      restart_policy:
        condition: "no"

volumes:
  ollama:
